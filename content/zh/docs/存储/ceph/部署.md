---
title: "部署"
---

# 环境说明

- **OS：**Centos 7.6
- **部署工具：**ceph-deploy
- **Ceph版本：**version 13.2.10（mimic）

**注意：**

- ceph-deploy 不再主动维护。它没有在比 Nautilus 更新的 Ceph 版本上进行测试。它不支持 RHEL8、CentOS 8 或更新的操作系统。



## 主机名 & 角色

- 注意：主机名必须与下面安装时设定的保持一致，否则初始MON节点的时候会报错

| Hostname          | IP        | Role                        |
| ----------------- | --------- | --------------------------- |
|                   | 10.0.0.7  | ceph-admin                  |
| stor01.azheng.com | 10.0.0.7  | storage、mon、osd、mgr、mds |
| stor02.azheng.com | 10.0.0.17 | storage、mon、osd、mgr      |
| stor03.azheng.com | 10.0.0.27 | storage、mon、osd、rgw      |



## 网络

- ceph 集群中通常需要包含两个网络：集群内部通信网络 & 外部公共通信网络
- 本次部署环境的网络规划为：
  - 集群内部通信网络：`10.0.0.0/24`
    - 10.0.0.7、10.0.0.17、10.0.0.27

  - 外部公共通信网络：`172.18.0.0/16`
    - 172.18.0.7、172.18.0.17、172.18.0.27




## 磁盘

- 每个节点都有一块100G的虚拟磁盘作为系统盘：`/dev/sda`
- 每个节点都有一块200G的虚拟磁盘作为OSD盘：`/dev/sdb`





# 先决条件

- 时间同步、关闭并禁用 SELinux、关闭 iptables & firewall、



## DNS 配置

- bind DNS，部署在10.0.0.8，系统为centos

```sh
# 安装bind DNS
# yum -y install bind bind-utils
# systemctl enable --now named.service


# 拷贝区域数据库文件
# cp -a /var/named/named.localhost /var/named/azheng.com.zone


# 编辑区域数据库文件
# vim /var/named/azheng.com.zone
$TTL 600
@ IN SOA ns1 mail.azheng.com. (
    2015042201;
    1H;
    10M;
    1D;
    12H;
)
            IN NS         ns1
ns1         IN A          10.0.0.8

ceph-admin  IN A          10.0.0.7
stor01      IN A          10.0.0.7
stor02      IN A          10.0.0.17
stor03      IN A          10.0.0.27




# 编辑主配置文件
# vim /etc/named.conf
options {
...
    listen-on port 53 { localhost; }; # 监听在本机所有网卡的53端口
    allow-query     { any; }; # 允许所有主机查询
...


# 子配置文件配置域名
# vim /etc/named.rfc1912.zones
...
zone "azheng.com" IN {
    type master;
    file "azheng.com.zone";
};


# 检查配置文件语法
# named-checkconf 


# 检查区域数据库的语法，第一个xiangzheng.org表示在配置文件中定义的名称，第三段表示区域数据库文件路径
# named-checkzone azheng.com /var/named/azheng.com.zone


# 语法无误后，重载bind DNS
# rndc reload
systemctl restart named.service
```

### 所有主机指向 DNS Server

#### centos

```sh
# 修改网卡配置文件（直接修改/etc/resolv.conf文件的话重启后会失效）
# vim /etc/sysconfig/network-scripts/ifcfg-eth0
...
DNS1=10.0.0.8
...


# 重载网卡配置
# nmcli connection reload
# nmcli connection up eth0


# 验证配置是否生效
# cat /etc/resolv.conf 
# Generated by NetworkManager
search azheng.com
nameserver 10.0.0.8


# 测试
# host ceph-admin.azheng.com;for i in {1..3};do host stor0${i}.azheng.com ; done
ceph-admin.azheng.com has address 10.0.0.7
stor01.azheng.com has address 10.0.0.7
stor02.azheng.com has address 10.0.0.17
stor03.azheng.com has address 10.0.0.27
# 简写:
# host ceph-admin ; for i in {1..3};do host stor0${i}; done
ceph-admin.azheng.com has address 10.0.0.7
stor01.azheng.com has address 10.0.0.7
stor02.azheng.com has address 10.0.0.17
stor03.azheng.com has address 10.0.0.27
```

#### Ubuntu

```sh
# 修改网卡配置文件（直接修改/etc/resolv.conf文件的话重启后会失效）
# vim /etc/netplan/00-installer-config.yaml
...
network:
  ethernets:
    eth0:
      addresses: [10.0.0.104/24]
      gateway4: 10.0.0.2
      nameservers:
              addresses: [10.0.0.8] # 指向DNS服务器
              search: []
  version: 2

...


# 重载网卡配置
# netplan apply


# 测试
# host ns1.azheng.com
ns1.azheng.com has address 10.0.0.8
```



## 准备 ceph 仓库

- ceph 官方仓库：http://download.ceph.com/
- ceph 阿里镜像仓库：https://mirrors.aliyun.com/ceph/
- ceph 的版本按首字母顺序排名，例如：kraken、luminous、mimic等（首字母排序越靠后则版本越新）；
- 程序包位于相关版本的noarch目录下，例如：rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm，负责生成适用于部署mimic版本Ceph的yum仓库配置文件，因此直接在线安装此程序包，也能直接生成yum仓库的相关配置。

```sh
# 在ceph-admin节点安装
# rpm -ivh https://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm


# 验证
# cat /etc/yum.repos.d/ceph.repo 
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-mimic/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
...
```



## 准备 ceph 账号

- 在**各节点**创建 cephadm 账号

```sh
# 创建账号
# useradd cephadm


# 设置密码
# echo "azheng" | passwd --stdin cephadm
```

- 在**各节点**定义sudo权限，以确保这些节点上新创建的cephadm用户都有无密码运行sudo命令的权限。

```sh
# echo "cephadm ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/cephadm


# chmod 0440 /etc/sudoers.d/cephadm
```



## 配置 ssh 免密登录

- ceph-deploy命令不支持运行中途的密码输入，因此，必须在管理节点（ceph-admin.azheng.com）上生成SSH密钥并将其公钥分发至Ceph集群的各节点上。

```sh
# 在ceph-admin节点上以cephadm用户的身份生成SSH密钥对
# su - cephadm
# ssh-keygen -t rsa -P ""
...


# 在ceph-admin节点上把公钥拷贝到各Ceph节点
# ssh-copy-id -i .ssh/id_rsa.pub cephadm@stor01.azheng.com
# ssh-copy-id -i .ssh/id_rsa.pub cephadm@stor02.azheng.com
# ssh-copy-id -i .ssh/id_rsa.pub cephadm@stor03.azheng.com
```



## 安装 ceph-deploy

- 在管理节点安装 ceph-deploy；
- Ceph存储集群的部署的过程可通过管理节点使用ceph-deploy全程进行，这里首先在管理节点安装ceph-deploy及其依赖到的程序包，其中有些包来自epel源

```sh
sudo yum install -y ceph-deploy python-setuptools python2-subprocess32
```





# 部署 RADOS 存储集群

## 初始化 RADOS 存储集群

### 1

- **首先在管理节点上以cephadm用户创建集群相关的配置文件目录**

```sh
# mkdir ceph-cluster

# cd ceph-cluster
```

### 2

- **初始化 Monitor 节点配置信息**

- 初始化第一个MON节点的命令格式为 `ceph-deploy new {initial-monitor-node(s)}`；
- 本示例中，stor01 即为第一个 Monitor 节点名称，其名称必须与节点当前实际使用的主机名称保存一致。运行如下命令即可生成初始配置：

```sh
# pwd
/home/cephadm/ceph-cluster


# 初始化，--cluster-network 指定集群内部通信网络，--public-network 指定外部公共通信网络
# ceph-deploy new --cluster-network 10.0.0.0/24 --public-network 172.18.0.0/16 stor01 stor02 stor03


# 验证生成的文件
# ls
ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring
# cat ceph.conf 
[global]
fsid = 23af67b8-235b-48f2-8dfe-8b52370a7419
public_network = 172.18.0.0/16
cluster_network = 10.0.0.0/24
mon_initial_members = stor01, stor02, stor03
mon_host = 172.18.0.7,172.18.0.17,172.18.0.27
auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
```

### 3

- **安装Ceph集群**

#### 全自动安装

- 这种安装方式因为是串行，因此在主机过多时会安装缓慢、网络超时等情况

```sh
# 因为定义了与后缀自动补全，因此此处主机名简写
# ceph-deploy install stor01 stor02 stor03
```

#### 各节点手动安装

```sh
# sudo rpm -ivh https://mirrors.aliyun.com/ceph/rpm-mimic/el7/noarch/ceph-release-1-1.el7.noarch.rpm


# sudo yum install -y ceph ceph-radosgw


# 因为在ceph-deploy阶段会生成一些元数据，因此各节点安装完成后还需在cephadm节点执行一遍此命令，但会很快执行完毕
# --no-adjust-repos 不再重新安装一遍
# ceph-deploy install --no-adjust-repos stor01 stor02 stor03
```

### 4

- **配置初始MON节点，并收集所有密钥**

```sh
# ceph-deploy mon create-initial


# 验证
# pwd
/home/cephadm/ceph-cluster
# ls -1
ceph.bootstrap-mds.keyring
ceph.bootstrap-mgr.keyring
ceph.bootstrap-osd.keyring
ceph.bootstrap-rgw.keyring
ceph.client.admin.keyring # 此文件ceph客户端管理所需的密钥，一定要保存好
ceph.conf
ceph-deploy-ceph.log
ceph.mon.keyring


# 验证，所有被部署的节点都会启动以下进程
# ps aux | grep ceph-mon
ceph        7757  0.0  4.2 477612 42500 ?        Ssl  20:52   0:04 /usr/bin/ceph-mon -f --cluster ceph --id stor01 --setuser ceph --setgroup ceph

# 还会监听6789端口
# ss -ntlp|grep 6789
LISTEN     0      128    172.18.0.7:6789                     *:*                   users:(("ceph-mon",pid=6893,fd=28))
```

### 5

- **把配置文件和admin密钥拷贝Ceph集群各节点，以免得每次执行”ceph“命令行时不得不明确指定MON节点地址和ceph.client.admin.keyring：**
  - 注意：通常只需要将密钥拷贝到控制端即可

```sh
# ceph-deploy admin stor01 stor02 stor03


# 验证，所有被拷贝密钥的节点都会生成此文件
# ls -l /etc/ceph/ceph.client.admin.keyring
-rw------- 1 root root 151 Nov 19 21:53 /etc/ceph/ceph.client.admin.keyring
```

- **而后在Ceph集群中需要运行ceph命令的的节点上（或所有节点上）以root用户的身份设定用户cephadm能够读取/etc/ceph/ceph.client.admin.keyring文件：**

```sh
setfacl -m u:cephadm:r /etc/ceph/ceph.client.admin.keyring
```

### 6

- **配置Manager节点，启动ceph-mgr进程（仅Luminious+版本需要，否则将认为集群不完整）**

```sh
# ceph-deploy mgr create stor01 stor02


# 验证，所有被部署的节点都会启动以下进程
# ps aux | grep ceph-mgr
ceph        7783  1.5  8.5 657968 85188 ?        Ssl  21:57   0:01 /usr/bin/ceph-mgr -f --cluster ceph --id stor02 --setuser ceph --setgroup ceph
```

### 7

- **在Ceph集群内的节点上以cephadm用户的身份运行如下命令，测试集群的健康状态**
- 此命令来自 `ceph-common` 包，通常在管理节点已经默认安装；

```sh
# ceph health
HEALTH_WARN OSD count 0 < osd_pool_default_size 3

# 详细信息
# ceph -s
  cluster:
    id:     23af67b8-235b-48f2-8dfe-8b52370a7419
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3 # OSD为0，需要进行下面的向RADOS 集群添加 OSD
 
  services:
    mon: 3 daemons, quorum stor01,stor02,stor03
    mgr: stor01(active), standbys: stor02
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     
```





## 向 RADOS 集群添加 OSD

### 列出 & 擦净磁盘

- **列出磁盘**

- `ceph-deploy disk` 命令可以检查并列出OSD节点上所有可用的磁盘的相关信息：

```sh
# pwd
/home/cephadm/ceph-cluster


# ceph-deploy disk list stor01 stor02 stor03
```

- **擦净磁盘**

- 而后，在管理节点上使用ceph-deploy命令擦除计划专用于OSD磁盘上的所有分区表和数据以便用于OSD
  - 命令格式为：`ceph-deploy disk zap {osd-server-name} {disk-name}`
  - **注意：此步会清除目标设备上的所有数据**
  - 提示：若设备上此前有数据，则可能需要在相应节点上以root用户使用 `ceph-volume lvm zap --destroy {DEVICE}` 命令进行；

```sh
# pwd
/home/cephadm/ceph-cluster

# 分别擦净stor01、stor02和stor03上用于OSD的一个磁盘设备vdb：
# ceph-deploy disk zap stor01 /dev/sdb
# ceph-deploy disk zap stor02 /dev/sdb
# ceph-deploy disk zap stor03 /dev/sdb
```

### 添加 OSD

- 默认使用的存储引擎为bluestore。
- 如下命令即可分别把stor01、stor02和stor03上的设备sdb添加为OSD：

```sh
# pwd
/home/cephadm/ceph-cluster

# ceph-deploy osd create stor01 --data /dev/sdb
# ceph-deploy osd create stor02 --data /dev/sdb
# ceph-deploy osd create stor03 --data /dev/sdb


# 所有被部署的节点都会启动以下进程
# ps aux | grep ceph-osd
ceph        9105  0.3  4.2 839752 42236 ?        Ssl  08:58   0:09 /usr/bin/ceph-osd -f --cluster ceph --id 0 --setuser ceph --setgroup ceph
```

### 验证 OSD

```sh
# pwd
/home/cephadm/ceph-cluster

# 列出指定节点上的OSD
# ceph-deploy osd list stor01 stor02 stor03
...

---

# 事实上，管理员也可以使用ceph命令查看OSD的相关信息
# ceph osd stat


---

# 或者使用如下命令了解相关的信息
# ceph osd dump
# ceph osd ls


---

# OK了
# ceph health
HEALTH_OK

# 详细信息
# ceph -s
  cluster:
    id:     23af67b8-235b-48f2-8dfe-8b52370a7419
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum stor01,stor02,stor03
    mgr: stor01(active), standbys: stor02
    osd: 3 osds: 3 up, 3 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   3.0 GiB used, 597 GiB / 600 GiB avail # 总共600G
    pgs:      
```



## 从RADOS集群中移除OSD的方法

Ceph集群中的一个OSD通常对应于一个设备，且运行于专用的守护进程。在某OSD设备出现故障，或管理员出于管理之需确实要移除特定的OSD设备时，需要先停止相关的守护进程，而后再进行移除操作。

### 对于Luminous及其之后的版本

1. 停用设备：ceph osd out {osd-num}

2. 停止进程：sudo systemctl stop ceph-osd@{osd-num}

3. 移除设备：ceph osd purge {id} --yes-i-really-mean-it

若类似如下的OSD的配置信息存在于ceph.conf配置文件中，管理员在删除OSD之后还需手动将其删除。

- ```
  [osd.1] host = {hostname}
  ```

### 对于Luminous之前的版本

管理员需要依次手动执行如下步骤删除OSD设备：

1. 于CRUSH运行图中移除设备：ceph osd crush remove {name}

2. 移除OSD的认证key：ceph auth del osd.{osd-num}

3. 最后移除OSD设备：ceph osd rm {osd-num}



# 测试 Ceph Cluster

## 创建存储池

存取数据时，客户端必须首先连接至RADOS集群上某存储池，而后根据对象名称由相关的CRUSH规则完成数据对象寻址。于是，为了测试集群的数据存取功能，这里首先创建一个用于测试的存储池`mypool`，并设定其PG数量为16个。

```sh
# ceph osd pool create mypool 16
pool 'mypool' created
```

## 列出存储池

```sh
# ceph osd pool ls
mypool


# rados lspools
mypool
```



## 上传数据对象

而后即可将测试文件上传至存储池中，例如下面的`rados put`命令将 `/etc/issue` 文件上传至`mypool` 存储池，对象名称依然保留为文件名 issue，而 `rados ls`命令则可以列出指定存储池中的数据对象。

```sh
# rados put issue /etc/issue --pool=mypool

# rados ls --pool=mypool
issue
```

## 查看数据对象

```sh
# 获取到存储池中数据对象的具体位置信息
# ceph osd map mypool issue
osdmap e32 pool 'mypool' (1) object 'issue' -> pg 1.651f88da (1.a) -> up ([2,1,0], p2) acting ([2,1,0], p2)
```

## 删除数据对象

```sh
# rados rm issue --pool=mypool
```

## 删除存储池

删除存储池命令存在数据丢失的风险，Ceph于是默认禁止此类操作。管理员需要在ceph.conf配置文件中启用支持删除存储池的操作后，方可使用类似如下命令删除存储池。

```sh
# ceph osd pool rm mypool mypool --yes-i-really-really-mean-it
```



# ---



# 扩展 Ceph 集群

- **以下扩展仅作参考，因为以下扩展的节点已经在上面创建时已经直接指定了**

## 扩展 Monitor 节点

Ceph存储集群需要至少运行一个Ceph Monitor和一个Ceph Manager，生产环境中，为了实现高可用性，Ceph存储集群通常运行多个监视器，以免单监视器整个存储集群崩溃。Ceph使用Paxos算法，该算法需要半数以上的监视器（大于n/2，其中n为总监视器数量）才能形成法定人数。尽管此非必需，但奇数个监视器往往更好。

“ceph-deploy mon add {ceph-nodes}”命令可以一次添加一个监视器节点到集群中。例如，下面的命令可以将集群中的stor02和stor03也运行为监视器节点：

```sh
[cephadm@ceph-admin ceph-cluster]$ ceph-deploy mon add stor02
[cephadm@ceph-admin ceph-cluster]$ ceph-deploy mon add stor03
```

设置完成后，可以在ceph客户端上查看监视器及法定人数的相关状态：

```json
# ceph quorum_status --format json-pretty

{
    "election_epoch": 26,
    "quorum": [
        0,
        1,
        2
    ],
    "quorum_names": [
        "stor01",
        "stor02",
        "stor03"
    ],
    "quorum_leader_name": "stor01",
    "monmap": {
        "epoch": 1,
        "fsid": "23af67b8-235b-48f2-8dfe-8b52370a7419",
        "modified": "2022-11-19 20:52:37.402539",
        "created": "2022-11-19 20:52:37.402539",
        "features": {
            "persistent": [
                "kraken",
                "luminous",
                "mimic",
                "osdmap-prune"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "stor01",
                "addr": "172.18.0.7:6789/0",
                "public_addr": "172.18.0.7:6789/0"
            },
            {
                "rank": 1,
                "name": "stor02",
                "addr": "172.18.0.17:6789/0",
                "public_addr": "172.18.0.17:6789/0"
            },
            {
                "rank": 2,
                "name": "stor03",
                "addr": "172.18.0.27:6789/0",
                "public_addr": "172.18.0.27:6789/0"
            }
        ]
    }
}
```



## 扩展 Manager 节点

Ceph Manager守护进程以“Active/Standby”模式运行，部署其它ceph-mgr守护程序可确保在Active节点或其上的ceph-mgr守护进程故障时，其中的一个Standby实例可以在不中断服务的情况下接管其任务。

`ceph-deploy mgr create {new-manager-nodes}`命令可以一次添加多个Manager节点。下面的命令可以将stor02节点作为备用的Manager运行：

```sh
[cephadm@ceph-admin ceph-cluster]$ ceph-deploy mgr create stor02
```





# ---

# Ceph 存储集群的访问接口

- **通常只启用一种接口来使用，如果未启用下面的接口，那么客户端则只能通过librados API 来访问RADOS集群**

## 启用 RDB

### 创建

```sh
# 创建一个名为rbddata的存储池，64为pg数量
# ceph osd pool create rbddata 64
pool 'rbddata' created


# 验证创建的存储池
# ceph osd pool ls
...
rbddata


# 在新建的存储池中启用rdb功能
# ceph osd pool application enable rbddata rbd
enabled application 'rbd' on pool 'rbddata'


# 将新建的存储池初始化为rdb可用的存储池
# rbd pool init -p rbddata
```

不过，rbd存储池并不能直接用于块设备，而是需要事先在其中按需创建映像（image），并把映像文件作为块设备使用：

```sh
# 在rbddata存储池中，创建一个名为img1，大小为1G的image（每一个image就是一个块设备）
# rbd create img1 --size 1024 --pool rbddata
```

### 验证

映像的相关的信息则可以使用`rbd info`命令获取：

```sh
# rbd info rbddata/img1
rbd image 'img1':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects) # 默认每个块为4M
	id: 3ba2a6b8b4567
	block_name_prefix: rbd_data.3ba2a6b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features: 
	flags: 
	create_timestamp: Sun Nov 20 21:51:45 2022


# 使用此命令格式输出效果一致
# rbd --image img1 --pool rbddata info
rbd image 'img1':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	id: 3ba2a6b8b4567
	block_name_prefix: rbd_data.3ba2a6b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features: 
	flags: 
	create_timestamp: Sun Nov 20 21:51:45 2022
```





## 启用 RADOSGW 接口

RGW提供的是REST接口，客户端通过http与其进行交互以完成数据的增删改查等管理操作。

RGW并非必须的接口，仅在需要用到与S3和Swift兼容的RESTful接口时才需要部署RGW实例，相关的命令为`ceph-deploy rgw create {gateway-node}`

### 创建

```sh
# pwd
/home/cephadm/ceph-cluster


# 把stor01部署为rgw主机
# ceph-deploy rgw create stor01
```

### 验证

```sh
# 在部署的节点上会生成radosgw的守护进程，并且默认会监听在TCP/7480端口
# ps aux | grep radosgw
ceph        9593  0.7  4.2 5055324 42352 ?       Ssl  22:26   0:00 /usr/bin/radosgw -f --cluster ceph --name client.rgw.stor01 --setuser ceph --setgroup ceph
# ss -ntlp | grep 7480
LISTEN     0      128          *:7480                     *:*                   users:(("radosgw",pid=9593,fd=40))



# ceph -s
  cluster:
    id:     23af67b8-235b-48f2-8dfe-8b52370a7419
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum stor01,stor02,stor03
    mgr: stor01(active), standbys: stor02
    osd: 3 osds: 3 up, 3 in
    rgw: 1 daemon active # 一个rgw的守护进程处于活跃状态
 
  data:
    pools:   6 pools, 112 pgs
    objects: 192  objects, 1.3 KiB
    usage:   3.0 GiB used, 597 GiB / 600 GiB avail
    pgs:     112 active+clean


# 访问集群内部网络的7480端口
# curl http://10.0.0.7:7480/
<?xml version="1.0" encoding="UTF-8"?><ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult>
```



默认情况下，RGW实例监听于TCP协议的7480端口，需要算定时，可以通过在运行RGW的节点上编辑其主配置文件ceph.conf进行修改，相关参数如下所示：

```sh
[client]
rgw_frontends = "civetweb port=8080"
```

而后需要重启相关的服务，命令格式为`systemctl restart ceph-radosgw@rgw.{node-name}`，例如重启stor03上的RGW，可以以root用户运行如下命令：

```sh
# systemctl status ceph-radosgw@rgw.stor01
```

RGW会在rados集群上生成包括如下存储池的一系列存储池：

```sh
# ceph osd pool ls
.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
```



## 启用 CephFS 接口

客户端通过内核中的cephfs文件系统接口即可挂载使用cephfs文件系统，或者通过FUSE接口与文件系统进行交互。

CephFS需要至少运行一个元数据服务器（MDS）守护进程（ceph-mds），此进程管理与CephFS上存储的文件相关的元数据，并协调对Ceph存储集群的访问。因此，若要使用CephFS接口，需要在存储集群中至少部署一个MDS实例。

### 创建

- 在stor01上启用MDS

```sh
# pwd
/home/cephadm/ceph-cluster


# ceph-deploy mds create stor01


# 验证：
# 会启动mds相关的service以及守护进程
# systemctl is-active ceph-mds@stor01
active
# ps aux | grep ceph-mds
ceph        8472  0.1  1.8 374236 18816 ?        Ssl  12:22   0:00 /usr/bin/ceph-mds -f --cluster ceph --id stor01 --setuser ceph --setgroup ceph
```

- 查看MDS的相关状态可以发现，刚添加的MDS处于Standby模式


```sh
# ceph mds stat
, 1 up:standby
```

- 为CephFS创建元数据和数据存储池，创建CephFS时为其分别指定元数据和数据相关的存储池

```sh
# 创建名为cephfs-metadata的存储池，作为元数据存储池
# ceph osd pool create cephfs-metadata 64

# 创建名为cephfs-data的存储池，作为数据存储池
# ceph osd pool create cephfs-data 64

# 指定元数据和数据存储池来创建CephFS
# ceph fs new cephfs cephfs-metadata cephfs-data
```



### 验证

```sh
# 查看cephfs文件系统的相关状态
# ceph fs status cephfs
cephfs - 0 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | stor01 | Reqs:    0 /s |   10  |   13  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs-metadata | metadata | 2286  |  188G |
|   cephfs-data   |   data   |    0  |  188G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
+-------------+
MDS version: ceph version 13.2.10 (564bdc4ae87418a232fc901524470e1a0f76d641) mimic (stable)


# 查看mds的状态，可以看到已经发生了改变
# ceph mds stat
cephfs-1/1/1 up  {0=stor01=up:active}
```



