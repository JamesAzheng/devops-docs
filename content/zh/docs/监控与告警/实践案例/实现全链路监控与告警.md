---
title: "实现全链路监控与告警"
---

## 一、项目背景

## 二、前期调研
### 1. 部署方式选型
- xxx

### 2. 告警系统选型
在生产环境中，没有绝对的“最好”，只有“最适合”。通常分为以下三种流派：

**方案 A：Prometheus + Alertmanager（GitOps）**
- **概述：** 不使用任何 Web 界面配置告警。所有的告警规则（PrometheusRules）和通知路由（Alertmanager Config）都写成 YAML 文件，存放在 Git 仓库中。通过 K8s Operator 自动加载。
- **适合场景：** 适合 Kubernetes 重度用户、DevOps 能力强、追求“一切皆代码”的团队。
- **优点：** 版本控制、易于回滚、通过 CI/CD 自动化管理，极其稳定。
- **缺点：** 门槛高，改个阈值需要提交代码，开发人员通常没权限改，需运维介入。

**方案 B：Grafana**
- **概述：** 直接在 Grafana 的 Web 界面上对着图表配置告警。
- **适合场景：** 适合中小型团队、数据源类型杂（既有 SQL 又有 Prometheus）、希望快速上手。
- **优点：** 直观，配置非常快，支持多数据源（比如你可以根据 MySQL 的订单量下跌报警，这是 Prometheus 做不到的）。
- **缺点：** 告警规则容易“散落”在各个 Dashboard 里，难以统一管理；Grafana 服务挂了，告警就全挂了（单点风险）。

**方案 C：夜莺 Nightingale**
- **概述：** 夜莺内置了类似 Alertmanager 的功能，而且界面更好用。底层部署 Prometheus 或 VictoriaMetrics 存数据，上层部署夜莺。所有的图表查看、告警规则配置、告警屏蔽、电话/短信通知，全在夜莺里操作。
- **适合场景：** 适合国内中大型企业、有专职 SRE 团队、需要分部门管理告警、需要屏蔽/抑制/排班功能的团队。
- **优点：** 解决了原生 Prometheus 难以管理（改 YAML 痛苦）和 Grafana 权限管理弱的问题。支持告警自愈、告警协同（类似 Jira 的工单流）。
- **缺点：** 成本高，需要部署和维护夜莺。

**总结：**
| 特性 | Alertmanager | Grafana Alerting | 夜莺 (Nightingale) |
| --- | --- | --- | --- |
| **核心能力** | 告警路由、静默、分组 | 可视化定义告警、多数据源支持 | 统一管理、多租户、排班、屏蔽 |
| **界面友好度** | 低（无配置 UI） | 高（所见即所得） | 极高（符合国内交互习惯） |
| **维护成本** | 低（单一二进制） | 中 | 高（不仅是组件，而是一个系统，依赖 MySQL/Redis） |
| **典型依赖** | 依赖 Prometheus | 独立，但需数据源 | 依赖 TSDB (Prom/VM) |
- 云原生的标准答案：**Prometheus Operator + Alertmanager**上手难，但符合 GitOps 理念，可进行版本控制、易于回滚、通过 CI/CD 自动化管理。
- 团队人少：直接用 **Grafana**。
- 团队人多（有开发、运维、测试区分），且需要对告警进行复杂的排班（比如半夜电话叫醒）：**夜莺**是目前的最佳选择。

## 三、架构概览


## 四、监控

### 1. 部署 kube-prometheus-stack

#### （1）下载 kube-prometheus-stack Chart
- 内网机无法下载的情况下，可从其他机器下载，然后 scp 到内网机。
```sh
# 添加仓库
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# 下载指定版本的压缩包
cd /root/k8s/helm/kube-prometheus-stack
helm pull prometheus-community/kube-prometheus-stack --version 68.5.0
```

{{% alert title="<i class='fa-solid fa-exclamation-triangle pe-1'></i> 注意事项" color=warning %}}
**执行以下命令，查看指定的 kube-prometheus-stack 版本，是否支持目前的 kubernetes 版本。**
```sh
root@localhost:~# helm show chart prometheus-community/kube-prometheus-stack --version 68.5.0 | grep kubeVersion
kubeVersion: '>=1.19.0-0'
```
{{% /alert %}}


#### （2）将 kube-prometheus-stack 相关镜像推送到 Harbor
**第一步：准备 Harbor 环境**
- 登录 Harbor，创建一个 Project，名字叫 external（或 public）。
- 将该 Project 的访问级别设为 Public（如果是内网且不敏感），或者创建一个 Robot Account 获取 pull token。

{{% alert title="<i class='fa-solid fa-info-circle pe-1'></i> 提示" color="info" %}}
- 可以将所有非本公司自研的、从 DockerHub/Quay.io 拉取的公共镜像，统统丢进去。
- 优点是管理方便，只需要给这个 Project 开启“公开访问”或者配置一个通用的 Pull Secret，所有 K8s 命名空间都能拉取。
- 也可以建一个 monitoring 项目。但这样以后还得建 logging、ingress 等等，项目会变得很多。对于中小规模团队，大杂烩 external 是最高效的。
{{% /alert %}}


**第二步：执行以下命令，获取镜像列表**
```sh
# 渲染模板并提取所有 image 字段，去掉引号和前缀，排序并去重，最后导出到 images.list 文件
helm template ./kube-prometheus-stack-68.5.0.tgz | grep -oE '[a-zA-Z0-9.-]+/[a-zA-Z0-9_./-]+:[a-zA-Z0-9_.-]+' | sort | uniq > images.list

# 手动添加 busybox 镜像，因为它在 kube-prometheus-stack 中被引用，但不在 Helm 模板中。
echo 'docker.io/library/busybox:1.31.1' >> images.list

# 查看提取出来的镜像列表
cat images.list
```

**第三步：执行批量处理脚本 (Pull -> Tag -> Push)**
```sh {filename="harbor-push.sh"}
#!/bin/bash

# --- 配置 ---
HARBOR_DOMAIN="harbor.bj.internal.example.com" # 需添加到 /etc/docker/daemon.json 的 insecure-registries 中，K8S 节点同理。
PROJECT="external"
IMAGE_LIST="images.list"

# --- 登录 Harbor (如果需要) ---
# docker login $HARBOR_DOMAIN

echo ">>> 开始处理镜像流..."

while read -r original_image; do
    # 忽略空行
    [ -z "$original_image" ] && continue
    
    echo "------------------------------------------------"
    echo "正在处理: $original_image"
    
    # 1. 拉取原始镜像
    docker pull $original_image
    
    # 2. 计算新的镜像名
    # 逻辑：提取镜像名称（去除路径），加上我们的 Harbor 前缀
    # 例如: quay.io/prometheus/node-exporter:v1.8.2 -> harbor.internal.example.com/external/prometheus/node-exporter:v1.8.2
    # 以 / 为分隔符，取第2列及之后的所有内容。
    image_name_tag=$(echo $original_image | cut -d'/' -f2-)
    new_image="${HARBOR_DOMAIN}/${PROJECT}/${image_name_tag}"
    
    echo "重命名为: $new_image"
    
    # 3. 打标签
    docker tag $original_image $new_image
    
    # 4. 推送到内网 Harbor
    docker push $new_image

done < "$IMAGE_LIST"

echo ">>> 处理完成！"
```

#### （3）在 Ingress 层为 Prometheus 创建 Basic Auth 凭证
{{% alert title="<i class='fa-solid fa-exclamation-triangle pe-1'></i> 注意事项" color=warning %}}
在生产环境中，Prometheus 的 Web UI 必须受到保护，否则任何人都可以查询敏感指标甚至删除数据。

如果使用的是 Traefik (ingressClassName: traefik-internal) 作为 Ingress Controller，最佳实践是利用 Traefik 的 中间件 (Middleware) 机制在 Ingress 层做拦截。

为什么选择 Ingress 层认证而不是 Prometheus 自带认证？
- 不破坏内部监控：如果开启 Prometheus 自身的内部认证，K8s 集群内的组件（如 Operator 自身、Sidecar）抓取数据时都需要配置密码，这会极大增加复杂度，弄不好监控就断了。
- 配置简单：只在入口处（Ingress）设卡，集群内部依然畅通。
{{% /alert %}}


**第一步：生成账号密码**
- 假设账号是 admin，密码是 prometheus2026：
```sh
# 安装 htpasswd 工具 (如果未安装)
apt install apache2-utils

# 生成加密的 auth 字符串
htpasswd -nb admin P@ssw0rd

# 输出示例：
admin:$apr1$oL8svGuT$AwC48EqklR/qt3/YddnXp0

# 生成 Base64 编码
echo -n 'admin:$apr1$oL8svGuT$AwC48EqklR/qt3/YddnXp0' | base64

# 输出示例：
YWRtaW46JGFwcjEkb0w4c3ZHdVQkQXdDNDhFcWtsUi9xdDMvWWRkblhwMA==
```

**第二步：创建认证 Secret 和 Traefik Middleware**
- 在 K8s 里创建一个 Secret 存密码，并创建一个 Traefik 的 Middleware 对象来引用这个 Secret。
```yaml {filename="/root/k8s/manifests/kube-prometheus-stack/auth-middleware.yaml"}
# 1. 存储密码的 Secret
apiVersion: v1
kind: Secret
metadata:
  name: prometheus-basic-auth
  namespace: kube-prometheus-stack # 必须和 Prometheus 在同一个命名空间
type: Opaque
data:
  # 将生成的 Base64 编码填在这里
  users: YWRtaW46JGFwcjEkb0w4c3ZHdVQkQXdDNDhFcWtsUi9xdDMvWWRkblhwMA== 

---

# 2. Traefik 的中间件定义
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: prometheus-basic-auth
  namespace: kube-prometheus-stack
spec:
  basicAuth:
    secret: prometheus-basic-auth
```

- 应用：
```sh
kubectl apply -f /root/k8s/manifests/kube-prometheus-stack/auth-middleware.yaml
```



#### （4）修改 values 文件
```yaml {filename="/root/k8s/helm/kube-prometheus-stack/values-prod.yaml"}
# ---------------------------------------------------------
# 配置文件: values-prod.yaml
# 适配 Chart 版本: kube-prometheus-stack-68.5.0
# 用途: 关闭 Alertmanager，数据源对接夜莺
# 安装/更新命令：
# helm upgrade --install kube-prometheus-stack ./kube-prometheus-stack-68.5.0.tgz --namespace kube-prometheus-stack --create-namespace -f values-prod.yaml
# ---------------------------------------------------------

global:
  imageRegistry: "harbor.bj.internal.example.com/external"

# 关闭 Alertmanager，因为告警规则管理和发送将由夜莺 (Nightingale) 接管
alertmanager:
  enabled: false

# Grafana 配置 (保留用于看详细的大盘)
grafana:
  enabled: true
  # 配置 admin 密码（随后从 grafana UI修改）
  adminPassword: "admin"
  ingress:
    enabled: true
    ingressClassName: traefik-internal
    annotations:
      # 让 cert-manager 自动根据下方 hosts 生成证书
      cert-manager.io/cluster-issuer: "root-ca" 
    hosts:
      - grafana.bj.internal.example.com
    tls:
      - secretName: grafana-tls # cert-manager 会自动创建这个 Secret
        hosts:
          - grafana.bj.internal.example.com
  persistence:
    enabled: true
    size: 20Gi
    storageClassName: "nfs-client"

# Prometheus 核心配置
prometheus:
  ingress:
    enabled: true
    ingressClassName: traefik-internal
    annotations:
      cert-manager.io/cluster-issuer: "root-ca"
      # 引用 Traefik Basic Auth 中间件，格式：<命名空间>-<中间件名称>@kubernetescrd
      traefik.ingress.kubernetes.io/router.middlewares: "kube-prometheus-stack-prometheus-basic-auth@kubernetescrd"
    hosts:
      - prometheus.bj.internal.example.com
    paths:
      - /
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus.bj.internal.example.com
  prometheusSpec:
    # 数据保留时间：因为未来可能接入 VictoriaMetrics，这里设短一点节省 K8s 存储，或者设长一点作为单机存储
    retention: 15d
    # 资源限制 (根据机器配置调整，防止 OOM)
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "4Gi"
        cpu: "2000m"

    # 关键配置：允许发现所有命名空间的 ServiceMonitor
    # 默认只发现安装该 Chart 的命名空间，改为 false 后，夜莺生成的规则或其他 namespace 的规则也能被抓到
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    ruleSelectorNilUsesHelmValues: false

    # 追加 Prometheus 抓取配置，专门用于监控非 K8s 的外部目标
    additionalScrapeConfigs:
      - job_name: 'bind-dns'
        scrape_interval: 15s
        metrics_path: /metrics
        static_configs:
          - targets:
            - '172.16.0.223:9119'
            labels:
              env: 'prod'
              service: 'dns'
              role: 'master'
          - targets:
            - '172.16.0.225:9119'
            labels:
              env: 'prod'
              service: 'dns'
              role: 'slave'

    # 持久化存储
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: "nfs-client"
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
  service:
    type: ClusterIP

# 关闭默认的 K8s 告警规则 (可选)
# 如果你想让夜莺全权管理告警，可以设为 false；
# 如果想保留 Prometheus 计算能力，只是不发告警（只看状态），设为 true
defaultRules:
  create: true
```
- 应用
```sh
cd /root/k8s/helm/kube-prometheus-stack
helm upgrade --install kube-prometheus-stack ./kube-prometheus-stack-68.5.0.tgz --namespace kube-prometheus-stack --create-namespace -f values-prod.yaml
```

#### （5）访问测试
- 访问 Prometheus 界面：https://prometheus.bj.internal.example.com
- 访问 Grafana 界面：http://grafana.bj.internal.example.com


### 2. 监控细分
#### （1）ESXi

#### （2）黑盒监控



#### （3）MySQL

#### （4）Redis

#### （5）Kafka

#### （6）Elasticsearch



## 五、告警

### 1. 部署夜莺
二进制部署，是夜莺官方最推荐的方式，systemd 托管，开机自启动，挂了自动拉起，也可以方便配置 CPU 限制，使用 journalctl 看日志，日志自动有滚动处理，稳，升级也方便，老运维应该深有同感。
- 参考文档：https://flashcat.cloud/docs/content/flashcat-monitor/nightingale-v7/install/intro/
```sh
# 下载安装包
wget https://github.com/ccfos/nightingale/releases/download/v8.5.0/n9e-v8.5.0-linux-amd64.tar.gz

# 创建安装目录
mkdir /opt/n9e

# 解压安装包
tar zxvf n9e-v8.5.0-linux-amd64.tar.gz -C /opt/n9e

# 创建 systemd 服务文件
cat > /etc/systemd/system/n9e.service <<EOF
[Unit]
Description=Nightingale Monitoring Service
After=network.target
[Service]
Type=simple
ExecStart=/opt/n9e/n9e
WorkingDirectory=/opt/n9e
Restart=always
RestartSec=5
StandardOutput=syslog
StandardError=syslog
SyslogIdentifier=n9e
[Install]
WantedBy=multi-user.target
EOF

# 修改配置文件 /opt/n9e/etc/config.toml 中 MySQL 与 redis 的指向地址与账号密码
[DB]
...
[Redis]
...

# 重启服务
systemctl daemon-reload
systemctl enable --now n9e

# 查看运行日志
journalctl -u n9e -f
```

### 2. 添加数据源
#### Prometheus
- 登录夜莺 Web 界面，点击“集成中心” -> “数据源” -> “新增” -> “Prometheus”
- 数据源名称：k8s-prometheus
- URL：https://prometheus.bj.internal.example.com/
- 跳过 SSL 验证：勾选
- 用户名密码：输入 Base64 编码前的用户名密码

### 3. 添加告警规则
{{% alert title="<i class='fa-solid fa-exclamation-triangle pe-1'></i> 注意事项" color="warning" %}}
在夜莺的架构中，Prometheus 仅作为 数据源（TSDB）来提供时序数据，而**告警规则的存储、评估和触发是由夜莺自身（n9e-server）来管理的。**

因此，**夜莺不会主动去读取或同步 Prometheus 配置文件中的告警规则。**
{{% /alert %}}


#### 方案一：使用夜莺内置的规则模板
- 在夜莺 Web 界面 -> 告警 -> 规则管理 -> 导入 -> 选择“导入内置告警规则”
- 选择需要导入的规则模板，如“Prometheus 告警规则”，点击“导入”

#### 方案二：导入现有的 Prometheus 告警规则

- 在夜莺 Web 界面 -> 告警 -> 规则管理 -> 导入 -> 选择 “导入Prometheus 告警规则” 后贴入yaml，如以下示例：

```yaml
groups:
- name: Bind9_Alerts
  rules:
  # 1. 服务宕机 (P0 - 紧急)
  - alert: DNS服务宕机
    expr: bind_up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "DNS 实例 {{ $labels.instance }} 已宕机"
      description: "BIND9 服务监控指标显示当前状态为 DOWN，服务已停止运行。"

  # 2. 主从序列号不一致 (P0 - 紧急)
  - alert: DNS主从同步失败
    expr: |
      abs(
        bind_zone_serial{instance="172.16.0.223:9119", zone_name="internal.example.com"} 
        - on(zone_name) 
        bind_zone_serial{instance="172.16.0.225:9119", zone_name="internal.example.com"}
      ) > 0
    for: 15m
    labels:
      severity: critical
    annotations:
      summary: "DNS 区域 {{ $labels.zone_name }} 主从同步失败"
      description: "检测到主节点与从节点的序列号 (Serial) 不一致已持续超过 15 分钟，请检查区域传输 (AXFR/IXFR) 状态。"

  # 3. 解析失败率过高 (P1 - 严重)
  - alert: DNS解析失败率过高
    expr: |
      sum(rate(bind_response_rcodes_total{rcode="SERVFAIL"}[5m])) by (instance) 
      / 
      sum(rate(bind_responses_total[5m])) by (instance) > 0.1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "DNS 实例 {{ $labels.instance }} 解析失败率过高"
      description: "该实例在过去 5 分钟内超过 10% 的请求返回了 SERVFAIL 错误，DNS 服务可能不可用或上游故障。"

  # 4. 递归解析大量超时 (P1 - 警告)
  - alert: DNS递归查询超时激增
    expr: rate(bind_resolver_query_errors_total{error="QueryTimeout"}[5m]) > 1
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "DNS 递归查询超时激增"
      description: "实例 {{ $labels.instance }} 向外网/上游 DNS 进行递归查询时出现大量超时，请检查网络连接。"

  # 5. 区域传输连续失败 (P1 - 警告)
  - alert: DNS区域传输失败
    expr: rate(bind_zone_transfer_failure_total[10m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "DNS 区域传输失败"
      description: "从节点 (Slave) {{ $labels.instance }} 尝试从主节点拉取区域数据失败，请检查主节点日志或网络防火墙。"
```


#### 告警规则示例截图
![](/docs/监控与告警/实践案例/告警规则示例截图.png)



### 4. 配置告警自愈
- 在夜莺 Web 界面 -> 告警 -> 自愈管理 -> 新增
- 自愈名称：根据告警规则名称命名，如“DNS服务宕机”
- 触发条件：选择“告警规则”，并选择对应的告警规则
- 执行操作：选择“执行shell命令”，输入自愈脚本，如重启 Bind9 服务
- 保存

---

### 5. 告警常见问题与解决方案
**告警事件中存在告警，但未触发通知**
- 检查是否配置报警媒介，并进行测试。以及是否在告警规则中配置了正确的通知规则。

---

**告警通知中显示的时间戳为UTC时间，而不是本地时间**
- 告警通知中显示的时间戳为UTC时间，而不是本地时间。
- 如果需要显示本地时间，需要在告警规则中配置 `time_format: local`。

## 六、展示
